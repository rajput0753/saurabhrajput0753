dbutils.fs.mount(
  source="wasbs://retail@retailers123456.blob.core.windows.net",
  mount_point="/mnt/retail_project",
  extra_configs={
    "fs.azure.account.key.retailers123456.blob.core.windows.net": "dP8jwqJMcBfP+ei+9hSQt8Je7sIyh9yiQU6z/MVvyIL9PWICqtCQ7w0hGB1F4U4DyDG0WZcTvmbc+AStAzzw9A=="
  }
)

dbutils.fs.ls('/mnt/retail_project/silver')

# COMMAND ----------

dbutils.fs.ls('/mnt/retail_project/bronze/')

# Read raw data from Bronze layer

#1
df_transactions = spark.read.format('parquet').load('/mnt/retail_project/bronze/transaction/')
display(df_transactions)
#2
df_product = spark.read.format('parquet').load('/mnt/retail_project/bronze/product/')
display(df_product)
#3
df_store = spark.read.format('parquet').load('/mnt/retail_project/bronze/store/')
display(df_store) 
#4
df_customers = spark.read.parquet('/mnt/retail_project/bronze/customer/rajput0753/saurabhrajput0753/refs/heads/main/')
display(df_customers)



create silver 

from pyspark.sql.functions import col

# Convert types and clean data
df_transactions = df_transactions.select(
    col("transaction_id").cast("int"),
    col("customer_id").cast("int"),
    col("product_id").cast("int"),
    col("store_id").cast("int"),
    col("quantity").cast("int"),
    col("transaction_date").cast("date")
)

df_product = df_product.select(
    col("product_id").cast("int"),
    col("product_name"),
    col("category"),
    col("price").cast("double")
)

df_store = df_store.select(
    col("store_id").cast("int"),
    col("store_name"),
    col("location")
)

df_customers = df_customers.select(
    "customer_id", "first_name", "last_name", "email", "city", "registration_date"
).dropDuplicates(["customer_id"])

# Join all data
df_silver = df_transactions \
    .join(df_customers, "customer_id") \
    .join(df_product, "product_id") \
    .join(df_store, "store_id") \
    .withColumn("total_amount", col("quantity") * col("price"))

display(df_silver)


dump to adls location 
silver_path = "/mnt/retail_project/silver/"
df_silver.write.mode("overwrite").format("delta").save(silver_path)
display(df_silver)

create silver dataset

spark.sql("CREATE SCHEMA IF NOT EXISTS retail_db")

df_silver.write.mode("append").saveAsTable("retail_db.retail_silver_cleaned")

%sql
select * from retail_db.retail_silver_cleaned

gold layer 

silver_df = spark.read.format("delta").load(silver_path)
display(silver_df)


from pyspark.sql.functions import sum, countDistinct, avg

gold_df = silver_df.groupBy(
    "transaction_date",
    "product_id", "product_name", "category",
    "store_id", "store_name", "location"
).agg(
    sum("quantity").alias("total_quantity_sold"),
    sum("total_amount").alias("total_sales_amount"),
    countDistinct("transaction_id").alias("number_of_transactions"),
    avg("total_amount").alias("average_transaction_value")
)

display(gold_df)

gold_path = "/mnt/retail_project/gold/"

gold_df.write.mode("overwrite").format("delta").save(gold_path)





